#
# Copyright (c) 2023-present, ETRI, All rights reserved.
#
#
# This is a PoC that inferences a model using IR generated by FX compile.
#


import torch
from torch import Tensor
from torch.nn.parameter import Parameter, UninitializedParameter
from torch.nn import init
import torch.nn as nn
from torch.optim import Adam
from torch import fx
from torch.fx.node import Node
import time



import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

torch.manual_seed(42)

batch_size = 64
in_features = 5120
out_features = 5120
hidden = 5120


class TestModel(nn.Module):
    def __init__(self):
        super().__init__()

        self.linear1 = nn.Linear(in_features, hidden)
        self.linear2 = nn.ModuleList()
        for i in range(2):
            self.linear2.append(nn.Linear(hidden, hidden))

        self.linear3 = nn.ModuleList()
        for i in range(2):
            self.linear3.append(nn.Linear(hidden, hidden))

        self.linear4 = nn.ModuleList()
        for i in range(2):
            self.linear4.append(nn.Linear(hidden, hidden))

        self.linear5 = nn.ModuleList()
        for i in range(2):
            self.linear5.append(nn.Linear(hidden, hidden))
        self.linear6 = nn.Linear(hidden, out_features)
        self.relu = nn.ReLU(inplace = True)

    def forward(self, x):
        x = self.relu(self.linear1(x))
        for m in self.linear2:
            x = self.relu(m(x))
        for m in self.linear3:
            x = self.relu(m(x))
        for m in self.linear4:
            x = self.relu(m(x))
        for m in self.linear5:
            x = self.relu(m(x))
        x = self.linear6(x)
        x = self.relu(x)
        return x

t1 = TestModel()

#
print(t1)

gm = fx.symbolic_trace(t1)

#
if isinstance(gm, fx.GraphModule):
    print(f"GraphModule ...")
else:
    print(f"Not GraphModule ...")

#
for node in gm.graph.nodes:
    print(f"node.op:{node.op}, node.name:{node.name}, node.target:{node.target}, node.args:{node.args}, node.all_input_nodes:{node.all_input_nodes}")

print("-------------")
print(gm.code)


tick =  time.time()

sample_input = torch.rand(batch_size, in_features)


class FXRun:

    def __init__(self, mod):
        self.mod = mod
        self.graph = mod.graph
        self.modules = dict(self.mod.named_modules())

    def fx_forward(self, *args):
        args_iter = iter(args)
        env: Dict[str, Node] = {}

        for node in self.graph.nodes:
            if node.op == 'placeholder':
                result = next(args_iter)

            elif node.op == 'get_attr':
                target_atoms = node.target.split('.')
                attr_itr = self.mod
                for i , atom in enumerate(target_atoms):
                    if not hasattr(attr_itr, atom):
                        raise RuntimeError(\
                                f"Node referenced nonexistant target{'.'.join(target_atoms[:i])}")
                    attr_itr = getattr(attr_itr, atom)
                result = attr_itr

            elif node.op == 'call_function':
                result = node.target(\
                        *fx.graph.map_arg(node.args, lambda n: env[n.name]), \
                        **fx.graph.map_arg(node.kwargs, lambda n: env[n.name]))

            elif node.op == 'call_method':
                self_obj, *args = fx.graph.map_arg(node.args, lambda n: env[n.name])
                kwargs = fx.graph.map_arg(node.kwargs, lambda n: env[n.name])
                result = getattr(self_obj, node.target)(*args, **kwargs)

            elif node.op == 'call_module':
                result = self.modules[node.target](\
                        *fx.graph.map_arg(node.args, lambda n: env[n.name]),\
                        **fx.graph.map_arg(node.kwargs, lambda n: env[n.name]))

            env[node.name] = result

        return fx.graph.map_arg(env[node.name], lambda n: env[n.name])

fx_run = FXRun(gm)

output1 = fx_run.fx_forward(sample_input) # actual

output2 = t1(sample_input) # expected
"""
output1 = gm(sample_input) # actual

output2 = t1(sample_input) # expected
"""

torch.testing.assert_close(output1, output2)
#torch.allclose(output1, output2)
#assert output1 == output2

tock = time.time()
elapsed_time = tock - tick
print('Time elapsed: %.3f sec ' % (elapsed_time))
print(output1)
print("#######")
print(output2)
