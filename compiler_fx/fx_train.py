#
# Copyright (c) 2023-present, ETRI, All rights reserved.
#
#
# This is a PoC that trains a model using IR generated by FX compile.
#    forward  --> using FX IR
#    backward --> using original Pytorch mechanism
#


import torch
from torch import Tensor
from torch.nn.parameter import Parameter, UninitializedParameter
from torch.nn import init
import torch.nn as nn
from torch.optim import Adam
from torch import fx
from torch.fx.node import Node
import copy
import time



import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

torch.manual_seed(42)

batch_size = 64
in_features = 5120
out_features = 5120
hidden = 5120


class TestModel(nn.Module):
    def __init__(self):
        super().__init__()

        self.linear1 = nn.Linear(in_features, hidden)
        self.linear2 = nn.ModuleList()
        for i in range(2):
            self.linear2.append(nn.Linear(hidden, hidden))

        self.linear3 = nn.ModuleList()
        for i in range(2):
            self.linear3.append(nn.Linear(hidden, hidden))

        self.linear4 = nn.ModuleList()
        for i in range(2):
            self.linear4.append(nn.Linear(hidden, hidden))

        self.linear5 = nn.ModuleList()
        for i in range(2):
            self.linear5.append(nn.Linear(hidden, hidden))
        self.linear6 = nn.Linear(hidden, out_features)
        self.relu = nn.ReLU(inplace = True)

    def forward(self, x):
        x = self.relu(self.linear1(x))
        for m in self.linear2:
            x = self.relu(m(x))
        for m in self.linear3:
            x = self.relu(m(x))
        for m in self.linear4:
            x = self.relu(m(x))
        for m in self.linear5:
            x = self.relu(m(x))
        x = self.linear6(x)
        x = self.relu(x)
        return x

t1 = TestModel()

t2 = copy.deepcopy(t1)

#
print(t1)
print("-----------------------")
print(t2)

gm1 = fx.symbolic_trace(t1)

for node in gm1.graph.nodes:
    print(f"node.op:{node.op}, node.target:{node.target}")

print("-------------")
print(gm1.code)
print("-------------")


class FXRun:

    def __init__(self, mod):
        self.mod = mod
        self.graph = mod.graph
        self.modules = dict(self.mod.named_modules())

    def fx_forward(self, *args):
        args_iter = iter(args)
        env: Dict[str, Node] = {}

        for node in self.graph.nodes:
            if node.op == 'placeholder':
                result = next(args_iter)

            elif node.op == 'get_attr':
                target_atoms = node.target.split('.')
                attr_itr = self.mod
                for i , atom in enumerate(target_atoms):
                    if not hasattr(attr_itr, atom):
                        raise RuntimeError(\
                                f"Node referenced nonexistant target{'.'.join(target_atoms[:i])}")
                    attr_itr = getattr(attr_itr, atom)
                result = attr_itr

            elif node.op == 'call_function':
                result = node.target(\
                        *fx.graph.map_arg(node.args, lambda n: env[n.name]), \
                        **fx.graph.map_arg(node.kwargs, lambda n: env[n.name]))

            elif node.op == 'call_method':
                self_obj, *args = fx.graph.map_arg(node.args, lambda n: env[n.name])
                kwargs = fx.graph.map_arg(node.kwargs, lambda n: env[n.name])
                result = getattr(self_obj, node.target)(*args, **kwargs)

            elif node.op == 'call_module':
                result = self.modules[node.target](\
                        *fx.graph.map_arg(node.args, lambda n: env[n.name]),\
                        **fx.graph.map_arg(node.kwargs, lambda n: env[n.name]))

            env[node.name] = result

        return fx.graph.map_arg(env[node.name], lambda n: env[n.name])




t1.train()
t2.train()
optimizer1 = Adam(t1.parameters(), lr=3e-5)
optimizer2 = Adam(t2.parameters(), lr=3e-5)

fx_run = FXRun(gm1)

tick =  time.time()

#sample_input = torch.rand(batch_size, in_features)
sample_output = torch.rand(batch_size, out_features)

for i in range(30):
    sample_input = torch.rand(batch_size, in_features)

    optimizer1.zero_grad()
    optimizer2.zero_grad()

    output1 = fx_run.fx_forward(sample_input) # actual
    loss1 = torch.nn.MSELoss()(output1, sample_output)
    loss1.backward()
    optimizer1.step()

    output2 = t2(sample_input) # expected
    loss2 = torch.nn.MSELoss()(output2, sample_output)
    loss2.backward()
    optimizer2.step()

    print(f'Step {i}, Loss1: {loss1}, Loss2: {loss2}')


torch.testing.assert_close(output1, output2)
#torch.allclose(output1, output2)
#assert output1 == output2

tock = time.time()
elapsed_time = tock - tick
print('Time elapsed: %.3f sec ' % (elapsed_time))
print(output1)
print("#######")
print(output2)
