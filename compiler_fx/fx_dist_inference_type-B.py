#
# Copyright (c) 2023-present, ETRI, All rights reserved.
#
#
#  This is a PoC that broadcasts the FX IR generated by FX compile 
#        to another machine, and then inferences in a pipeline style using N hosts.
#
#   In this PoC, a partition of FX IR is transferred to the corresponding host, and
#       distributed inference is performed based on the FX IR partition.
#
#
#
#  Sample Usage:
#      <machine #0>
#            torchrun --nproc_per_node=1 --nnodes=N --node_rank=0
#                  --master_addr="X.X.X.X" --master_port=29500 fx_dist_inference_type-B.py
#      <machine #1>
#            torchrun --nproc_per_node=1 --nnodes=N --node_rank=1
#                  --master_addr="X.X.X.X" --master_port=29500 fx_dist_inference_type-B.py
#
#          (...)
#
#      <machine #N-1>
#            torchrun --nproc_per_node=1 --nnodes=N --node_rank=N-1
#                  --master_addr="X.X.X.X" --master_port=29500 fx_dist_inference_type-B.py


import torch
from torch import Tensor
from torch.nn.parameter import Parameter, UninitializedParameter
from torch.nn import init
import torch.nn as nn
from torch.optim import Adam
from torch import fx
from torch.fx.node import Node
import time

import torch.distributed as dist
import datetime
import torch.distributed.rpc as rpc

from typing import Any, Dict, Iterator, List, Optional, Tuple, Union


import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from torch.fx.graph_module import GraphModule
from torch.fx.passes.split_module import split_module


torch.manual_seed(42)

#
# Total host count
#
#num_host=N
num_host=4  

batch_size = 64
in_features = 5120
out_features = 5120
hidden = 5120


class TestModel(nn.Module):
    def __init__(self):
        super().__init__()

        self.linear1 = nn.Linear(in_features, hidden)
        self.linear2 = nn.ModuleList()
        for i in range(2):
            self.linear2.append(nn.Linear(hidden, hidden))

        self.linear3 = nn.ModuleList()
        for i in range(2):
            self.linear3.append(nn.Linear(hidden, hidden))

        self.linear4 = nn.ModuleList()
        for i in range(2):
            self.linear4.append(nn.Linear(hidden, hidden))

        self.linear5 = nn.ModuleList()
        for i in range(2):
            self.linear5.append(nn.Linear(hidden, hidden))
        self.linear6 = nn.Linear(hidden, out_features)
        #self.relu = nn.ReLU(inplace = True)
        self.relu = nn.ReLU(inplace = False)

    def forward(self, x):
        x = self.relu(self.linear1(x))
        for m in self.linear2:
            x = self.relu(m(x))
        for m in self.linear3:
            x = self.relu(m(x))
        for m in self.linear4:
            x = self.relu(m(x))
        for m in self.linear5:
            x = self.relu(m(x))
        x = self.linear6(x)
        x = self.relu(x)
        return x


class Simple_split_test2(object):
    def __init__(self):
        self.initialize_comm()

        self.model_ir = []

    def initialize_comm(self):

        if dist.is_initialized():
            print(f"Communication already initialized")
            return


        self.rank = int(os.environ["RANK"])
        self.world_size = int(os.environ["WORLD_SIZE"])
        self.master_addr = os.getenv("MASTER_ADDR")
        self.master_port = os.getenv("MASTER_PORT")
        self.stage = 0

        #
        print(f" --- rank:{self.rank}, world_size:{self.world_size}, master:{self.master_addr}, port:{self.master_port}")

        self.backend = "gloo"
        init_method = "tcp://" + str(self.master_addr) + ":" + str(self.master_port)

        dist.init_process_group(backend=self.backend, rank=self.rank, world_size=self.world_size, init_method=init_method)

        #
        print(f" --- rank:{dist.get_rank()}, world_size:{dist.get_world_size()}")

        options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=10, rpc_timeout=30)

        rpc.init_rpc(f"worker{self.rank}", rank=self.rank, world_size=self.world_size, rpc_backend_options=options,)

        # rpc.shutdown()


    def simple_split(self, gm, t1, metadata_range):

        length = gm.graph.nodes.__len__()
        segment = length // num_host
        print(f"length:{length}, num_host:{num_host}, segment:{segment}")

        self.last_flag = False
        def part_fn(node):
            last_idx, last_name = metadata_range[-1]

            if self.last_flag == True:
                idx = last_idx
                #print(f" part_fn:  node.name:{node.name}, --> {idx}")
                return idx

            idx = 0

            cur = node
            while cur.name != last_name:
                for i, m_name in metadata_range:
                    if cur.name == m_name:
                        idx = i
                        #print(f" part_fn:  node.name:{node.name}, m_name:{m_name}, --> {idx}")
                        return idx

                cur = cur._next

            if cur.name == last_name:
                idx = last_idx
                self.last_flag = True

            #print(f" part_fn:  node.name:{node.name}, --> {idx}")
            return idx


        k, cnt = 0, 0
        for n in gm.graph.nodes:
            if n.op == 'call_module':
                cnt = cnt + 1

            if cnt == segment:
                metadata_range.append((k, n.name))
                k = k + 1
                cnt = 0

            if k > num_host - 1:
                break

        if len(metadata_range) <  num_host:
            metadata_range.append((k, n.name))

        print(metadata_range)

        submodules = split_module(gm, t1, part_fn, keep_original_order=True)

        return submodules


    def setup_pair_info(self):

        if self.rank == 0:
            self.rank_pair: Dict[int, List[int]] = {}
            rank_pair_obj = [self.rank_pair]

            for rank in range(self.world_size):
                if rank == 0:
                    continue
                self.rank_pair.setdefault(rank, [0, rank] )

            dist.broadcast_object_list(rank_pair_obj, src=0, device=self.device)

        else:
            self.rank_pair: Dict[int, List[int]] = {}

            rank_pair_obj = [None]
            dist.broadcast_object_list(rank_pair_obj, src=0, device=self.device)
            self.rank_pair = rank_pair_obj[0]

        print(f"## In rank[{self.rank}], setup_pair_info completed ==> rank_pair:{self.rank_pair}")


    def setup_ctrl_group(self):
        self.ctrl_group: Dict[int, Any] = {}

        for rank in range(self.world_size):
            if rank == 0:
                continue
            pair_ranks = self.rank_pair[rank]
            self.ctrl_group[rank] = dist.new_group(pair_ranks)

        print(f"##In rank[{self.rank}], setup_ctrl_group completed")

        
    def metadata_transfer2(self):

        self.metadata_range = []

        if self.rank == 0:
            t1 = TestModel()
            gm = fx.symbolic_trace(t1)

        self.device = torch.device("cpu")

        self.setup_pair_info()
        self.setup_ctrl_group()


        if self.rank == 0:
            submods = self.simple_split(gm, t1, self.metadata_range)

            skip = False
            to_rank = 0
            for submod in submods.modules():
                if skip == False and isinstance(submod, fx.GraphModule):
                    skip = True
                    continue
                if skip == True and isinstance(submod, fx.GraphModule):
                    print(f"submod:{submod._get_name()}")

                    if to_rank == 0:
                        print(f"### rank = 0 holding submod_0")

                        self.model_ir.append(submod)
                        self.model_ir.append(0)

                        #for node in submod.graph.nodes:
                        #    print(f"-- node.op:{node.op}, node.name:{node.name}, node.target:{node.target}, node.all_input_nodes:{node.all_input_nodes}")

                    else:
                        print(f"### send IR partition to rank:{to_rank}")
                        object_list = [submod, to_rank]
                        dist.broadcast_object_list(object_list, src=0, group=self.ctrl_group[to_rank], device=self.device)

                    to_rank = to_rank + 1

                    #print(f" >> FROM:{self.rank} ==> TO:{to_rank} FX IR partition transferred")

        else:

            object_list = [None,None]
            dist.broadcast_object_list(object_list, src=0, group=self.ctrl_group[self.rank], device=self.device)

            submod = object_list[0]

            self.model_ir.append(submod)
            self.stage = object_list[1]

            if submod is None:
                print(f"In rank[{self.rank}], FX IR sync failed")
            else:
                print(f" ### rank:{self.rank}, stage:{self.stage} received <==  FX IR partition")

                #print(f" ############## rank[{self.rank}], stage:{self.stage} #################")
                #for node in submod.graph.nodes:
                #    print(f"-- node.op:{node.op}, node.name:{node.name}, node.target:{node.target}, node.all_input_nodes:{node.all_input_nodes}")



class FXRun3:

    def __init__(self, split_info: Simple_split_test2, device):

        self.mod = split_info.model_ir[0]
        self.graph = self.mod.graph
        self.modules = dict(self.mod.named_modules())
        self.env: Dict[str, Node] = {}
        self.metadata_range = split_info.metadata_range
        self.rank = split_info.rank
        self.world_size = split_info.world_size
        self.device = device
        self.stage = split_info.stage

    def receive_activation(self, split_node_name, from_rank):

        dimension = torch.tensor([0], dtype=torch.long)
        dist.recv(dimension, from_rank)

        shape = torch.tensor([0] * dimension, dtype=torch.long)
        dist.recv(shape, from_rank)
        shape = tuple(shape.tolist())

        obj = torch.zeros(size=shape)
        dist.recv(obj, from_rank)

        return obj

    def send_activation(self, split_node_name, to_rank):

        obj = self.env[split_node_name]

        dimension = torch.tensor(len(obj.size()), dtype=torch.long) # ex. 2
        dist.send(dimension, to_rank)

        shape = torch.tensor(list(obj.size()), dtype=torch.long) # ex. [54, 5120]
        dist.send(shape, to_rank)

        dist.send(obj, to_rank)


    def print_range(self):
        print(f" # rank = {self.rank}, metadata_range:{self.metadata_range}")

        for node in self.mod.graph.nodes:
            print(f"-- node.op:{node.op}, node.name:{node.name}, node.target:{node.target}, node.args:{node.args}, node.all_input_nodes:{node.all_input_nodes}")



    def fx_forward3(self, *args):
        self.args_iter = iter(args)

        for n in self.mod.graph.nodes:
            from_ = n
            break

        for n in reversed(self.mod.graph.nodes):
            to_ = n
            break

        if self.rank > 0:
            split_node_name = "placeholder"
            pre_split_rank = self.rank - 1
            #print(f"## rank:{self.rank}, receive activation from {pre_split_rank}, split_node_name:{split_node_name}")
            self.env[split_node_name] = self.receive_activation(split_node_name, pre_split_rank)

        cur = from_
        while cur != to_:
            self.fx_ir_run_node2(cur)
            cur = cur._next
        result = self.fx_ir_run_node2(cur)

        #print(f" rank:{self.rank}, cur.node name{cur.name}, split_node_name:{to_.name}")

        if self.rank < self.world_size - 1:
            split_node_name = "output"
            next_split_rank = self.rank + 1
            #print(f"### rank:{self.rank} send activation to {next_split_rank}, split_node_name:{split_node_name}")
            self.send_activation(split_node_name, next_split_rank)

        return result


    def restore_env(self, node: Node) -> Tuple[Tuple, Dict]:
        #print(f"## before restore_env, node:{node}, node.args:{node.args}, node.kwargs:{node.kwargs}")

        args = fx.graph.map_arg(node.args, lambda n: self.env[n.name])
        assert isinstance(args, tuple)

        kwargs = fx.graph.map_arg(node.kwargs, lambda n: self.env[n.name])
        assert isinstance(kwargs, dict)

        #print(f">>> after restore_env, node:{node}, node.name:{node.name}, args:{args}, kwargs:{kwargs}")

        return args, kwargs
        

    def fx_ir_run_node2(self, node):

        args, kwargs = self.restore_env(node)

        if node.op == 'placeholder' and self.stage == 0:
            result = next(self.args_iter)

        elif node.op == 'placeholder' and self.stage != 0:
            result = self.env["placeholder"]

        elif node.op == 'get_attr':
            target_atoms = node.target.split('.')
            attr_itr = self.mod
            for i , atom in enumerate(target_atoms):
                if not hasattr(attr_itr, atom):
                    raise RuntimeError(\
                            f"Node referenced nonexistant target{'.'.join(target_atoms[:i])}")
                attr_itr = getattr(attr_itr, atom)
            result = attr_itr

        elif node.op == 'call_function':
            #result = node.target(\
            #        *fx.graph.map_arg(node.args, lambda n: self.env[n.name]), \
            #        **fx.graph.map_arg(node.kwargs, lambda n: self.env[n.name]))
            result = node.target(*args, *kwargs)

        elif node.op == 'call_method':
            #self_obj, *args = fx.graph.map_arg(node.args, lambda n: self.env[n.name])
            #kwargs = fx.graph.map_arg(node.kwargs, lambda n: self.env[n.name])
            #result = getattr(self_obj, node.target)(*args, **kwargs)

            self_obj = args[0]
            args = args[1:]
            result = getattr(self_obj, node.target)(*args, **kwargs)

        elif node.op == 'call_module':
            #result = self.modules[node.target](\
            #        *fx.graph.map_arg(node.args, lambda n: self.env[n.name]),\
            #        **fx.graph.map_arg(node.kwargs, lambda n: self.env[n.name]))
            result = self.modules[node.target](*args, **kwargs)

        elif node.op == 'output':
            #result = fx.graph.map_arg(node.args[0], lambda n: self.env[n.name])
            result =  args[0]

        #
        print(f" ## [rank:{sim_split.rank}], run - node:{node.name}, node.op:{node.op}")

        self.env[node.name] = result

        return result



sim_split = Simple_split_test2()
sim_split.metadata_transfer2()


if sim_split.rank == 0:
    sample_input = torch.rand(batch_size, in_features)
else:
    sample_input = None

fx_run3 = FXRun3(sim_split, sim_split.device)

fx_run3.print_range()

output1 = fx_run3.fx_forward3(sample_input)

if sim_split.rank == sim_split.world_size - 1:
    print(output1)
print(f"[rank:{sim_split.rank}], run completed ...")

rpc.shutdown()

